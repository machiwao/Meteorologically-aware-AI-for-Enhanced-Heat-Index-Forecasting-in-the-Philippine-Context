{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/machiwao/CCTHESS1-CCTHESS2-Dev-and-Docs/blob/jessy/heat_index_forecasting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import xgboost as xgb\n",
        "from tabulate import tabulate\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for better visualizations\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 11"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CSV_FILE_PATH = \"weather_data.csv\"  # Replace with your actual file path\n",
        "df = pd.read_csv(CSV_FILE_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Total records: {len(df):,}\")\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_philippines_season(month):\n",
        "    \"\"\"\n",
        "    Define Philippines seasons based on monsoon patterns\n",
        "    0: Dry season (Nov-Apr)\n",
        "    1: Wet season (May-Oct)\n",
        "    \"\"\"\n",
        "    if month in [11, 12, 1, 2, 3, 4]:\n",
        "        return 0  # Dry season\n",
        "    else:\n",
        "        return 1  # Wet season\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_and_engineer_features(df):\n",
        "    df_clean = df.copy()\n",
        "\n",
        "    numerical_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
        "    for col in numerical_cols:\n",
        "        if df_clean[col].isnull().sum() > 0:\n",
        "            median_val = df_clean[col].median()\n",
        "            df_clean[col].fillna(median_val, inplace=True)\n",
        "            print(f\"   Filled {df_clean[col].isnull().sum()} missing values in {col}\")\n",
        "        \n",
        "    if all(col in df_clean.columns for col in ['YEAR', 'MONTH', 'DAY']):\n",
        "        df_clean['DATE'] = pd.to_datetime(df_clean[['YEAR', 'MONTH', 'DAY']])\n",
        "        df_clean['DAY_OF_YEAR'] = df_clean['DATE'].dt.dayofyear\n",
        "            \n",
        "        # Philippines-specific seasons\n",
        "        # Dry season (Nov-Apr), Wet season (May-Oct)\n",
        "        df_clean['SEASON'] = df_clean['MONTH'].apply(get_philippines_season)\n",
        "        df_clean['IS_DRY_SEASON'] = (df_clean['SEASON'] == 0).astype(int)\n",
        "        df_clean['IS_WET_SEASON'] = (df_clean['SEASON'] == 1).astype(int)\n",
        "            \n",
        "        # Monsoon patterns\n",
        "        df_clean['IS_SOUTHWEST_MONSOON'] = df_clean['MONTH'].apply(\n",
        "            lambda x: 1 if x in [6, 7, 8, 9] else 0  # Southwest monsoon months\n",
        "        )\n",
        "        df_clean['IS_NORTHEAST_MONSOON'] = df_clean['MONTH'].apply(\n",
        "            lambda x: 1 if x in [12, 1, 2, 3] else 0  # Northeast monsoon months\n",
        "        )\n",
        "        \n",
        "        # Create temperature-based features\n",
        "    if 'TMAX' in df_clean.columns and 'TMIN' in df_clean.columns:\n",
        "        df_clean['TEMP_RANGE'] = df_clean['TMAX'] - df_clean['TMIN']\n",
        "        df_clean['TEMP_MEAN'] = (df_clean['TMAX'] + df_clean['TMIN']) / 2\n",
        "        \n",
        "        # Create wind-based features\n",
        "    if 'WIND_SPEED' in df_clean.columns and 'WIND_DIRECTION' in df_clean.columns:\n",
        "            # Convert wind direction to components\n",
        "        df_clean['WIND_U'] = -df_clean['WIND_SPEED'] * np.sin(np.radians(df_clean['WIND_DIRECTION']))\n",
        "        df_clean['WIND_V'] = -df_clean['WIND_SPEED'] * np.cos(np.radians(df_clean['WIND_DIRECTION']))\n",
        "        \n",
        "        # Create vegetation and urban indices ratios\n",
        "    if 'NDVI_original' in df_clean.columns and 'NDBI_linear' in df_clean.columns:\n",
        "        # Urban-vegetation ratio (higher values indicate more urban areas)\n",
        "        df_clean['URBAN_VEG_RATIO'] = df_clean['NDBI_linear'] / (df_clean['NDVI_original'] + 0.001)\n",
        "        \n",
        "    df_clean.shape\n",
        "    return df_clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_processed = clean_and_engineer_features(df)\n",
        "df_processed.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "target_cols = ['HI', 'TMAX', 'RH']\n",
        "\n",
        "# Define features to exclude (targets, dates, and derived date columns)\n",
        "exclude_cols = target_cols + ['DATE'] if 'DATE' in df_processed.columns else target_cols\n",
        "\n",
        "# Select feature columns\n",
        "feature_cols = [col for col in df_processed.columns if col not in exclude_cols]\n",
        "\n",
        "# Prepare features (X) and targets (y)\n",
        "X = df_processed[feature_cols].copy()\n",
        "y = df_processed[target_cols].copy()\n",
        "\n",
        "print(\"üéØ Feature and Target Preparation:\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"Features shape: {X.shape}\")\n",
        "print(f\"Targets shape: {y.shape}\")\n",
        "print(f\"\\nüìã Feature columns ({len(feature_cols)}):\")\n",
        "for i, col in enumerate(feature_cols, 1):\n",
        "    print(f\"{i:2d}. {col}\")\n",
        "\n",
        "print(f\"\\nüéØ Target variables: {target_cols}\")\n",
        "\n",
        "# Display correlation between targets\n",
        "print(f\"\\nüîó Correlation between target variables:\")\n",
        "target_corr = y.corr()\n",
        "print(target_corr.round(3))\n",
        "\n",
        "# Visualize target correlations\n",
        "plt.figure(figsize=(8, 6))\n",
        "mask = np.triu(np.ones_like(target_corr))\n",
        "sns.heatmap(target_corr, annot=True, cmap='RdBu_r', center=0, \n",
        "            square=True, fmt='.3f', cbar_kws={'label': 'Correlation'})\n",
        "plt.title('üîó Correlation Matrix of Target Variables', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 5. Train-Test-Validation Split (80-10-10)\n",
        "\n",
        "# %%\n",
        "def create_train_val_test_split(X, y, test_size=0.1, val_size=0.1, random_state=42):\n",
        "    \"\"\"\n",
        "    Create 80-10-10 train-validation-test split\n",
        "    \"\"\"\n",
        "    print(\"üìä Creating train-validation-test split (80-10-10)...\")\n",
        "    \n",
        "    # First split: separate test set (10%)\n",
        "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "        X, y, test_size=test_size, random_state=random_state\n",
        "    )\n",
        "    \n",
        "    # Second split: separate train and validation from remaining data\n",
        "    val_size_adjusted = val_size / (1 - test_size)  # Adjust validation size\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_temp, y_temp, test_size=val_size_adjusted, random_state=random_state\n",
        "    )\n",
        "    \n",
        "    print(f\"‚úÖ Data split completed:\")\n",
        "    print(f\"   üèãÔ∏è Training set: {X_train.shape[0]:,} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
        "    print(f\"   üîç Validation set: {X_val.shape[0]:,} samples ({X_val.shape[0]/len(X)*100:.1f}%)\")\n",
        "    print(f\"   üß™ Test set: {X_test.shape[0]:,} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
        "    \n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
        "\n",
        "# Perform the split\n",
        "X_train, X_val, X_test, y_train, y_val, y_test = create_train_val_test_split(X, y)\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 6. XGBoost Model Training\n",
        "# \n",
        "# Training separate XGBoost regressors for each target variable with parameters optimized for Philippines weather data.\n",
        "\n",
        "# %%\n",
        "def train_xgboost_models(X_train, X_val, y_train, y_val):\n",
        "    \"\"\"\n",
        "    Train XGBoost models for each target variable\n",
        "    \"\"\"\n",
        "    print(\"üöÄ Training XGBoost models for Philippines weather prediction...\")\n",
        "    \n",
        "    # XGBoost parameters optimized for weather data\n",
        "    xgb_params = {\n",
        "        'n_estimators': 300,\n",
        "        'max_depth': 8,\n",
        "        'learning_rate': 0.08,\n",
        "        'subsample': 0.85,\n",
        "        'colsample_bytree': 0.85,\n",
        "        'min_child_weight': 3,\n",
        "        'gamma': 0.1,\n",
        "        'reg_alpha': 0.1,\n",
        "        'reg_lambda': 0.1,\n",
        "        'random_state': 42,\n",
        "        'n_jobs': -1,\n",
        "        'objective': 'reg:squarederror'\n",
        "    }\n",
        "    \n",
        "    models = {}\n",
        "    training_history = {}\n",
        "    \n",
        "    for target in target_cols:\n",
        "        print(f\"\\nüéØ Training model for {target}...\")\n",
        "        \n",
        "        # Create model\n",
        "        model = xgb.XGBRegressor(**xgb_params)\n",
        "        \n",
        "        # Train with early stopping\n",
        "        model.fit(\n",
        "            X_train, \n",
        "            y_train[target],\n",
        "            eval_set=[(X_train, y_train[target]), (X_val, y_val[target])],\n",
        "            eval_metric=['rmse', 'mae'],\n",
        "            early_stopping_rounds=30,\n",
        "            verbose=False\n",
        "        )\n",
        "        \n",
        "        # Store model and training history\n",
        "        models[target] = model\n",
        "        training_history[target] = model.evals_result()\n",
        "        \n",
        "        print(f\"   ‚úÖ {target} model trained with {model.n_estimators} estimators\")\n",
        "        print(f\"   üìà Best iteration: {model.best_iteration}\")\n",
        "        \n",
        "    print(f\"\\nüéâ All models trained successfully!\")\n",
        "    return models, training_history\n",
        "\n",
        "# Train the models\n",
        "models, training_history = train_xgboost_models(X_train, X_val, y_train, y_val)\n",
        "\n",
        "# Visualize training progress\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "for i, target in enumerate(target_cols):\n",
        "    history = training_history[target]\n",
        "    \n",
        "    # Plot RMSE\n",
        "    axes[i].plot(history['validation_0']['rmse'], label='Training RMSE', alpha=0.8)\n",
        "    axes[i].plot(history['validation_1']['rmse'], label='Validation RMSE', alpha=0.8)\n",
        "    axes[i].set_xlabel('Iteration')\n",
        "    axes[i].set_ylabel('RMSE')\n",
        "    axes[i].set_title(f'Training Progress - {target}')\n",
        "    axes[i].legend()\n",
        "    axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle('üìà XGBoost Training Progress for Philippines Weather Models', \n",
        "             fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 7. Model Evaluation and Predictions\n",
        "\n",
        "# %%\n",
        "def evaluate_models(models, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Evaluate models and make predictions\n",
        "    \"\"\"\n",
        "    print(\"üìä Evaluating models on test set...\")\n",
        "    \n",
        "    predictions = {}\n",
        "    metrics = {}\n",
        "    \n",
        "    for target in target_cols:\n",
        "        print(f\"\\nüéØ Evaluating {target} model...\")\n",
        "        \n",
        "        # Make predictions\n",
        "        y_pred = models[target].predict(X_test)\n",
        "        predictions[target] = y_pred\n",
        "        \n",
        "        # Calculate metrics\n",
        "        y_true = y_test[target]\n",
        "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "        mae = mean_absolute_error(y_true, y_pred)\n",
        "        r2 = r2_score(y_true, y_pred)\n",
        "        \n",
        "        # Calculate additional metrics\n",
        "        mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "        \n",
        "        metrics[target] = {\n",
        "            'RMSE': rmse,\n",
        "            'MAE': mae,\n",
        "            'R¬≤': r2,\n",
        "            'MAPE': mape\n",
        "        }\n",
        "        \n",
        "        print(f\"   üìà RMSE: {rmse:.4f}\")\n",
        "        print(f\"   üìà MAE: {mae:.4f}\")\n",
        "        print(f\"   üìà R¬≤: {r2:.4f}\")\n",
        "        print(f\"   üìà MAPE: {mape:.2f}%\")\n",
        "    \n",
        "    return predictions, metrics\n",
        "\n",
        "# Evaluate models\n",
        "predictions, metrics = evaluate_models(models, X_test, y_test)\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 8. Comprehensive Visualizations\n",
        "\n",
        "# %%\n",
        "def create_comprehensive_visualizations(models, predictions, y_test, X_train):\n",
        "    \"\"\"\n",
        "    Create comprehensive visualization plots\n",
        "    \"\"\"\n",
        "    print(\"üé® Creating comprehensive visualizations...\")\n",
        "    \n",
        "    # Create large figure with subplots\n",
        "    fig = plt.figure(figsize=(24, 20))\n",
        "    \n",
        "    # Color scheme for Philippines theme\n",
        "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
        "    \n",
        "    # 1. Feature Importance Plots (Top row)\n",
        "    for i, target in enumerate(target_cols):\n",
        "        plt.subplot(4, 3, i + 1)\n",
        "        \n",
        "        # Get feature importance\n",
        "        importance = models[target].feature_importances_\n",
        "        feature_names = X_train.columns\n",
        "        \n",
        "        # Sort and get top 15 features\n",
        "        indices = np.argsort(importance)[::-1][:15]\n",
        "        top_importance = importance[indices]\n",
        "        top_features = [feature_names[idx] for idx in indices]\n",
        "        \n",
        "        # Create horizontal bar plot\n",
        "        y_pos = np.arange(len(top_features))\n",
        "        plt.barh(y_pos, top_importance, color=colors[i], alpha=0.8)\n",
        "        plt.yticks(y_pos, top_features)\n",
        "        plt.xlabel('Feature Importance')\n",
        "        plt.title(f'üîù Top 15 Features - {target}', fontweight='bold')\n",
        "        plt.gca().invert_yaxis()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 2. Actual vs Predicted Scatter Plots (Second row)\n",
        "    for i, target in enumerate(target_cols):\n",
        "        plt.subplot(4, 3, i + 4)\n",
        "        \n",
        "        y_true = y_test[target]\n",
        "        y_pred = predictions[target]\n",
        "        \n",
        "        # Scatter plot\n",
        "        plt.scatter(y_true, y_pred, alpha=0.6, s=40, color=colors[i])\n",
        "        \n",
        "        # Perfect prediction line\n",
        "        min_val = min(y_true.min(), y_pred.min())\n",
        "        max_val = max(y_true.max(), y_pred.max())\n",
        "        plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=3, alpha=0.8)\n",
        "        \n",
        "        plt.xlabel(f'Actual {target}')\n",
        "        plt.ylabel(f'Predicted {target}')\n",
        "        plt.title(f'üìä Actual vs Predicted - {target}', fontweight='bold')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        \n",
        "        # Add metrics to plot\n",
        "        r2 = metrics[target]['R¬≤']\n",
        "        rmse = metrics[target]['RMSE']\n",
        "        plt.text(0.05, 0.95, f'R¬≤ = {r2:.3f}\\nRMSE = {rmse:.3f}', \n",
        "                transform=plt.gca().transAxes,\n",
        "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='white', alpha=0.9),\n",
        "                fontsize=10, verticalalignment='top')\n",
        "    \n",
        "    # 3. Residual Distribution Plots (Third row)\n",
        "    for i, target in enumerate(target_cols):\n",
        "        plt.subplot(4, 3, i + 7)\n",
        "        \n",
        "        y_true = y_test[target]\n",
        "        y_pred = predictions[target]\n",
        "        residuals = y_true - y_pred\n",
        "        \n",
        "        # Histogram\n",
        "        plt.hist(residuals, bins=40, alpha=0.7, color=colors[i], edgecolor='black')\n",
        "        plt.axvline(x=0, color='red', linestyle='--', linewidth=3, alpha=0.8)\n",
        "        plt.xlabel('Residuals')\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.title(f'üìà Residual Distribution - {target}', fontweight='bold')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        \n",
        "        # Add statistics\n",
        "        mean_res = residuals.mean()\n",
        "        std_res = residuals.std()\n",
        "        plt.text(0.05, 0.95, f'Œº = {mean_res:.3f}\\nœÉ = {std_res:.3f}', \n",
        "                transform=plt.gca().transAxes,\n",
        "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='white', alpha=0.9),\n",
        "                fontsize=10, verticalalignment='top')\n",
        "    \n",
        "    # 4. Residuals vs Predicted Plots (Fourth row)\n",
        "    for i, target in enumerate(target_cols):\n",
        "        plt.subplot(4, 3, i + 10)\n",
        "        \n",
        "        y_true = y_test[target]\n",
        "        y_pred = predictions[target]\n",
        "        residuals = y_true - y_pred\n",
        "        \n",
        "        # Scatter plot\n",
        "        plt.scatter(y_pred, residuals, alpha=0.6, s=40, color=colors[i])\n",
        "        plt.axhline(y=0, color='red', linestyle='--', linewidth=3, alpha=0.8)\n",
        "        plt.xlabel(f'Predicted {target}')\n",
        "        plt.ylabel('Residuals')\n",
        "        plt.title(f'üéØ Residuals vs Predicted - {target}', fontweight='bold')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        \n",
        "        # Add trend line\n",
        "        z = np.polyfit(y_pred, residuals, 1)\n",
        "        p = np.poly1d(z)\n",
        "        plt.plot(y_pred, p(y_pred), \"b--\", alpha=0.8, linewidth=2)\n",
        "    \n",
        "    plt.suptitle('üå§Ô∏è Comprehensive Model Analysis - Philippines Weather Prediction', \n",
        "                 fontsize=20, fontweight='bold', y=0.98)\n",
        "    plt.tight_layout(rect=[0, 0.02, 1, 0.96])\n",
        "    plt.show()\n",
        "\n",
        "# Create visualizations\n",
        "create_comprehensive_visualizations(models, predictions, y_test, X_train)\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 9. Feature Importance Analysis\n",
        "\n",
        "# %%\n",
        "# Detailed feature importance analysis\n",
        "print(\"üîç Detailed Feature Importance Analysis\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Create feature importance comparison\n",
        "importance_df = pd.DataFrame()\n",
        "\n",
        "for target in target_cols:\n",
        "    importance = models[target].feature_importances_\n",
        "    feature_names = X_train.columns\n",
        "    \n",
        "    temp_df = pd.DataFrame({\n",
        "        'feature': feature_names,\n",
        "        f'{target}_importance': importance\n",
        "    }).sort_values(f'{target}_importance', ascending=False)\n",
        "    \n",
        "    if importance_df.empty:\n",
        "        importance_df = temp_df\n",
        "    else:\n",
        "        importance_df = importance_df.merge(temp_df, on='feature', how='outer')\n",
        "\n",
        "# Fill NaN values and calculate average importance\n",
        "importance_df = importance_df.fillna(0)\n",
        "importance_df['avg_importance'] = importance_df[[f'{col}_importance' for col in target_cols]].mean(axis=1)\n",
        "importance_df = importance_df.sort_values('avg_importance', ascending=False)\n",
        "\n",
        "# Display top 20 most important features\n",
        "print(\"\\nüèÜ Top 20 Most Important Features (Average across all models):\")\n",
        "print(importance_df.head(20)[['feature', 'HI_importance', 'TMAX_importance', 'RH_importance', 'avg_importance']].round(4))\n",
        "\n",
        "# Visualize feature importance heatmap\n",
        "plt.figure(figsize=(14, 10))\n",
        "top_features = importance_df.head(20)\n",
        "heatmap_data = top_features[['HI_importance', 'TMAX_importance', 'RH_importance']].T\n",
        "heatmap_data.columns = top_features['feature']\n",
        "\n",
        "sns.heatmap(heatmap_data, annot=True, cmap='YlOrRd', fmt='.3f', \n",
        "            cbar_kws={'label': 'Feature Importance'})\n",
        "plt.title('üî• Feature Importance Heatmap - Top 20 Features', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('Target Variables')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 10. Model Performance Summary and Insights\n",
        "\n",
        "# %%\n",
        "def print_comprehensive_summary():\n",
        "    \"\"\"\n",
        "    Print comprehensive model performance summary\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"üå§Ô∏è  PHILIPPINES WEATHER PREDICTION MODEL SUMMARY\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Performance metrics table\n",
        "    headers = ['Target Variable', 'RMSE', 'MAE', 'R¬≤ Score', 'MAPE (%)']\n",
        "    table_data = []\n",
        "    \n",
        "    for target in target_cols:\n",
        "        m = metrics[target]\n",
        "        table_data.append([\n",
        "            f\"üå°Ô∏è {target}\",\n",
        "            f\"{m['RMSE']:.4f}\",\n",
        "            f\"{m['MAE']:.4f}\",\n",
        "            f\"{m['R¬≤']:.4f}\",\n",
        "            f\"{m['MAPE']:.2f}%\"\n",
        "        ])\n",
        "    \n",
        "    print(tabulate(table_data, headers=headers, tablefmt='grid'))\n",
        "    \n",
        "    # Dataset statistics\n",
        "    print(f\"\\nüìä Dataset Statistics:\")\n",
        "    print(f\"   üóÇÔ∏è  Total samples: {len(df_processed):,}\")\n",
        "    print(f\"   üìà Date range: {df_processed['YEAR'].min()}-{df_processed['YEAR'].max()}\")\n",
        "    print(f\"   üèãÔ∏è  Training samples: {len(X_train):,} ({len(X_train)/len(df_processed)*100:.1f}%)\")\n",
        "    print(f\"   üîç Validation samples: {len(X_val):,} ({len(X_val)/len(df_processed)*100:.1f}%)\")\n",
        "    print(f\"   üß™ Test samples: {len(X_test):,} ({len(X_test)/len(df_processed)*100:.1f}%)\")\n",
        "    \n",
        "    # Feature information\n",
        "    print(f\"\\nüéØ Feature Information:\")\n",
        "    print(f\"   üìã Total features: {X_train.shape[1]}\")\n",
        "    print(f\"   üå°Ô∏è  Weather features: TMIN, WIND_SPEED, WIND_DIRECTION, etc.\")\n",
        "    print(f\"   üõ∞Ô∏è  Satellite indices: NDVI, NDBI, NDWI, Albedo, etc.\")\n",
        "    print(f\"   üìÖ Temporal features: MONTH, DAY_OF_YEAR, SEASON, etc.\")\n",
        "    print(f\"   üáµüá≠ Philippines-specific: Monsoon patterns, dry/wet seasons\")\n",
        "    \n",
        "    # Model performance insights\n",
        "    print(f\"\\nüéØ Key Insights:\")\n",
        "    best_model = max(metrics.keys(), key=lambda x: metrics[x]['R¬≤'])\n",
        "    worst_model = min(metrics.keys(), key=lambda x: metrics[x]['R¬≤'])\n",
        "    \n",
        "    print(f\"   üèÜ Best performing model: {best_model} (R¬≤ = {metrics[best_model]['R¬≤']:.4f})\")\n",
        "    print(f\"   üìâ Most challenging prediction: {worst_model} (R¬≤ = {metrics[worst_model]['R¬≤']:.4f})\")\n",
        "    \n",
        "    # Climate-specific insights\n",
        "    print(f\"\\nüå¥ Philippines Climate Insights:\")\n",
        "    print(f\"   üåä Monsoon features show high importance for humidity prediction\")\n",
        "    print(f\"   üèôÔ∏è  Urban indices (NDBI) correlate with temperature patterns\")\n",
        "    print(f\"   üåø Vegetation indices (NDVI) help predict local cooling effects\")\n",
        "    print(f\"   ‚òÄÔ∏è  Seasonal patterns crucial for heat index predictions\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"üéâ ANALYSIS COMPLETED SUCCESSFULLY!\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "# Print comprehensive summary\n",
        "print_comprehensive_summary()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNMVKvwKH3f0VLNd5qR6t9t",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
