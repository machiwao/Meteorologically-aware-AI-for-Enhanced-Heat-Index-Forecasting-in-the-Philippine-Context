{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b531cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from tabulate import tabulate\n",
    "import glob\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "import sys\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set max rows/cols for better console display\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af4dcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_philippines_season(month):\n",
    "    \"\"\"\n",
    "    Philippines seasons: 0=Dry (Nov-Apr), 1=Wet (May-Oct)\n",
    "    \"\"\"\n",
    "    return 0 if month in [11, 12, 1, 2, 3, 4] else 1\n",
    "\n",
    "def load_and_clean_data(data_directory=\"C:/Users/jessy/CCTHESS1-CCTHESS2-Dev-and-Docs/merged_datasets\"):\n",
    "    \"\"\"\n",
    "    Load, combine, clean, and feature engineer data from all PAGASA stations.\n",
    "    \"\"\"\n",
    "    print(\"Starting data loading and cleaning...\")\n",
    "    \n",
    "    all_data = []\n",
    "    required_date_cols = ['YEAR', 'MONTH', 'DAY']\n",
    "    \n",
    "    # Find all CSV files based on expected patterns\n",
    "    csv_patterns = [\"*Daily Data.csv\", \"*Daily_Data.csv\"]\n",
    "    csv_files = []\n",
    "    for pattern in csv_patterns:\n",
    "        csv_files.extend(glob.glob(os.path.join(data_directory, pattern)))\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(\"Error: No station data CSV files found. Please ensure files are in the expected directory.\")\n",
    "        sys.exit(1)\n",
    "        \n",
    "    for file_path in csv_files:\n",
    "        try:\n",
    "            filename = os.path.basename(file_path)\n",
    "            station_name = filename.replace(' Daily Data.csv', '').replace(' Daily_Data.csv', '')\n",
    "            \n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Check for required date columns\n",
    "            if not all(col in df.columns for col in required_date_cols):\n",
    "                 print(f\"Warning: Skipping {station_name}. Missing required date columns.\")\n",
    "                 continue\n",
    "\n",
    "            df['STATION'] = station_name\n",
    "            all_data.append(df)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {filename}: {str(e)}\")\n",
    "            \n",
    "    if not all_data:\n",
    "        raise ValueError(\"No valid station data could be loaded! Check file paths and content.\")\n",
    "    \n",
    "    df_combined = pd.concat(all_data, ignore_index=True)\n",
    "    df_clean = df_combined.copy()\n",
    "    \n",
    "    # ------------------\n",
    "    # Data Cleaning\n",
    "    # ------------------\n",
    "    target_cols = ['HI', 'TMAX', 'RH']\n",
    "    df_clean = df_clean.dropna(subset=target_cols, how='all')\n",
    "    \n",
    "    # Fill missing values: first by station median, then by overall median\n",
    "    numeric_cols = df_clean.select_dtypes(include=[np.number]).columns.drop(['YEAR', 'MONTH', 'DAY'], errors='ignore')\n",
    "    \n",
    "    for station in df_clean['STATION'].unique():\n",
    "        station_mask = df_clean['STATION'] == station\n",
    "        station_data = df_clean[station_mask]\n",
    "        for col in numeric_cols:\n",
    "            if col in df_clean.columns:\n",
    "                station_median = station_data[col].median()\n",
    "                if pd.notna(station_median):\n",
    "                    df_clean.loc[station_mask, col] = df_clean.loc[station_mask, col].fillna(station_median)\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if col in df_clean.columns and df_clean[col].isnull().sum() > 0:\n",
    "            overall_median = df_clean[col].median()\n",
    "            df_clean[col].fillna(overall_median, inplace=True)\n",
    "            \n",
    "    # ------------------\n",
    "    # Feature Engineering\n",
    "    # ------------------\n",
    "    \n",
    "    # Temporal Features\n",
    "    df_clean['DATE'] = pd.to_datetime(df_clean[['YEAR', 'MONTH', 'DAY']])\n",
    "    df_clean['DAY_OF_YEAR'] = df_clean['DATE'].dt.dayofyear\n",
    "    \n",
    "    df_clean['SEASON'] = df_clean['MONTH'].apply(get_philippines_season)\n",
    "    df_clean['IS_DRY_SEASON'] = (df_clean['SEASON'] == 0).astype(int)\n",
    "    df_clean['IS_WET_SEASON'] = (df_clean['SEASON'] == 1).astype(int)\n",
    "    df_clean['IS_SOUTHWEST_MONSOON'] = df_clean['MONTH'].apply(lambda x: 1 if x in [6, 7, 8, 9] else 0)\n",
    "    df_clean['IS_NORTHEAST_MONSOON'] = df_clean['MONTH'].apply(lambda x: 1 if x in [12, 1, 2, 3] else 0)\n",
    "    \n",
    "    # Derived Meteorological and GEE Features (if columns exist)\n",
    "    if 'TMAX' in df_clean.columns and 'TMIN' in df_clean.columns:\n",
    "        df_clean['TEMP_RANGE'] = df_clean['TMAX'] - df_clean['TMIN']\n",
    "        df_clean['TEMP_MEAN'] = (df_clean['TMAX'] + df_clean['TMIN']) / 2\n",
    "    \n",
    "    if 'WIND_SPEED' in df_clean.columns and 'WIND_DIRECTION' in df_clean.columns:\n",
    "        df_clean['WIND_U'] = -df_clean['WIND_SPEED'] * np.sin(np.radians(df_clean['WIND_DIRECTION']))\n",
    "        df_clean['WIND_V'] = -df_clean['WIND_SPEED'] * np.cos(np.radians(df_clean['WIND_DIRECTION']))\n",
    "\n",
    "    # Create vegetation/urban ratios from Google Earth Engine indices\n",
    "    if 'NDVI_original' in df_clean.columns and 'NDBI_linear' in df_clean.columns:\n",
    "        df_clean['URBAN_VEG_RATIO'] = df_clean['NDBI_linear'] / (df_clean['NDVI_original'] + 0.001)\n",
    "\n",
    "    # Sort data chronologically for time series split\n",
    "    df_clean = df_clean.sort_values(by='DATE').reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Data processing complete. Final records: {len(df_clean):,}\")\n",
    "    print(f\"Date Range: {df_clean['DATE'].min().date()} to {df_clean['DATE'].max().date()}\")\n",
    "    return df_clean\n",
    "\n",
    "# Execute data loading and cleaning\n",
    "try:\n",
    "    df_processed = load_and_clean_data()\n",
    "except Exception as e:\n",
    "    print(f\"FATAL SETUP ERROR: {e}\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d80e61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94092208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features_and_targets(df):\n",
    "    \"\"\"\n",
    "    Prepare feature matrix (X) and target variables (Y)\n",
    "    \"\"\"\n",
    "    print(\"Preparing features and targets...\")\n",
    "    \n",
    "    target_cols = ['HI', 'TMAX', 'RH']\n",
    "    \n",
    "    # Create station dummy variables\n",
    "    station_dummies = pd.get_dummies(df['STATION'], prefix='STATION', dummy_na=False)\n",
    "    df_with_stations = pd.concat([df, station_dummies], axis=1)\n",
    "    \n",
    "    # Define columns to exclude from features\n",
    "    exclude_cols = target_cols + ['DATE', 'STATION', 'YEAR', 'MONTH', 'DAY', 'SEASON']\n",
    "    \n",
    "    # Select feature columns\n",
    "    feature_cols = [col for col in df_with_stations.columns if col not in exclude_cols]\n",
    "    \n",
    "    # Prepare features and targets\n",
    "    X = df_with_stations[feature_cols].copy()\n",
    "    y = df_with_stations[target_cols].copy()\n",
    "    \n",
    "    # Remove any remaining non-numeric columns\n",
    "    X = X.select_dtypes(include=[np.number])\n",
    "    \n",
    "    print(f\"Features shape: {X.shape}\")\n",
    "    return X, y\n",
    "\n",
    "# Prepare features and targets\n",
    "X, y = prepare_features_and_targets(df_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7e96ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_series_split(X, y, train_pct=0.8, val_pct=0.1, test_pct=0.1):\n",
    "    \"\"\"\n",
    "    Create chronological 80-10-10 split (Train -> Validation -> Test).\n",
    "    Assumes X and y are already sorted by date.\n",
    "    \"\"\"\n",
    "    print(\"Creating chronological Train-Validation-Test split (80/10/10)...\")\n",
    "    \n",
    "    total_len = len(X)\n",
    "    train_end = int(total_len * train_pct)\n",
    "    val_end = int(total_len * (train_pct + val_pct))\n",
    "    \n",
    "    # Train set (first 80%)\n",
    "    X_train, y_train = X[:train_end], y[:train_end]\n",
    "    \n",
    "    # Validation set (next 10%)\n",
    "    X_val, y_val = X[train_end:val_end], y[train_end:val_end]\n",
    "    \n",
    "    # Test set (last 10%)\n",
    "    X_test, y_test = X[val_end:], y[val_end:]\n",
    "    \n",
    "    print(f\"Train samples: {len(X_train):,}\")\n",
    "    print(f\"Validation samples: {len(X_val):,}\")\n",
    "    print(f\"Test samples: {len(X_test):,}\")\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# Perform the chronological split\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = create_time_series_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793b237b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost_models(X_train, X_val, y_train, y_val):\n",
    "    \"\"\"\n",
    "    Train XGBoost models for HI, TMAX, and RH prediction\n",
    "    \"\"\"\n",
    "    print(\"\\nTraining XGBoost Models...\")\n",
    "    \n",
    "    xgb_params = {\n",
    "        'n_estimators': 500,\n",
    "        'max_depth': 8,\n",
    "        'learning_rate': 0.05,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'min_child_weight': 3,\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1,\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse',\n",
    "        'early_stopping_rounds': 50\n",
    "    }\n",
    "    \n",
    "    models = {}\n",
    "    target_cols = ['HI', 'TMAX', 'RH']\n",
    "    \n",
    "    for target in target_cols:\n",
    "        model = RandomForestRegressor(**xgb_params)\n",
    "        model.fit(\n",
    "            X_train, \n",
    "            y_train[target],\n",
    "            eval_set=[(X_val, y_val[target])],\n",
    "            verbose=False\n",
    "        )\n",
    "        models[target] = model\n",
    "        print(f\"Trained {target} model. Best iteration: {model.best_iteration}\")\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe12bf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(models, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluate trained models on the test set\n",
    "    \"\"\"\n",
    "    print(\"\\nEvaluating models on chronological test set...\")\n",
    "    \n",
    "    table_data = []\n",
    "    target_cols = ['HI', 'TMAX', 'RH']\n",
    "    \n",
    "    for target in target_cols:\n",
    "        y_pred = models[target].predict(X_test)\n",
    "        y_true = y_test[target]\n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        \n",
    "        table_data.append([\n",
    "            target,\n",
    "            f\"{rmse:.4f}\",\n",
    "            f\"{mae:.4f}\",\n",
    "            f\"{r2:.4f}\"\n",
    "        ])\n",
    "    \n",
    "    print(\"\\nModel Performance Metrics (Test Set):\")\n",
    "    print(tabulate(table_data, headers=['Target', 'RMSE', 'MAE', 'R²'], tablefmt='pipe'))\n",
    "\n",
    "# Train and evaluate models\n",
    "models = train_xgboost_models(X_train, X_val, y_train, y_val)\n",
    "evaluate_models(models, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099b2717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 5. Station-Specific Future Weather Prediction (Final Robust Logic)\n",
    "\n",
    "# %%\n",
    "def predict_next_days_by_station(models, df_processed, X_train, n_days=3):\n",
    "    \"\"\"\n",
    "    Predict weather for next n days for every station.\n",
    "    Uses station-specific median feature values from historical data.\n",
    "    \"\"\"\n",
    "    print(f\"\\nPredicting Weather for Next {n_days} Days (By Station)...\")\n",
    "    \n",
    "    target_cols = ['HI', 'TMAX', 'RH']\n",
    "    predictions_list = []\n",
    "    \n",
    "    current_date = datetime.now().date()\n",
    "    \n",
    "    # ---------------------------------------------\n",
    "    # 1. Pre-calculate station-specific median features\n",
    "    # ---------------------------------------------\n",
    "    \n",
    "    # Identify non-temporal, non-target features to get medians for\n",
    "    # We must exclude 'STATION' from this list to prevent errors in groupby\n",
    "    feature_cols_for_median = [\n",
    "        col for col in df_processed.columns \n",
    "        if col not in target_cols and \n",
    "           col not in ['DATE', 'STATION', 'YEAR', 'MONTH', 'DAY', 'SEASON']\n",
    "    ]\n",
    "    \n",
    "    # Calculate median for each feature for each station\n",
    "    # We must handle potential NaN/empty feature sets\n",
    "    station_medians = df_processed.groupby('STATION')[feature_cols_for_median].median()\n",
    "    station_medians_dict = station_medians.T.to_dict()\n",
    "    \n",
    "    # Get the list of all station names and model's dummy columns\n",
    "    station_names = station_medians.index.unique() # Use only stations that have median features\n",
    "    station_cols = [col for col in X_train.columns if col.startswith('STATION_')]\n",
    "\n",
    "    # ---------------------------------------------\n",
    "    # 2. Prediction Loop\n",
    "    # ---------------------------------------------\n",
    "    for day_offset in range(n_days):\n",
    "        target_date = current_date + timedelta(days=day_offset)\n",
    "        date_str = target_date.strftime(\"%Y-%m-%d\")\n",
    "        month = target_date.month\n",
    "\n",
    "        temporal_features = {\n",
    "            'DAY_OF_YEAR': target_date.timetuple().tm_yday,\n",
    "            'IS_DRY_SEASON': 1 if get_philippines_season(month) == 0 else 0,\n",
    "            'IS_WET_SEASON': 1 if get_philippines_season(month) == 1 else 0,\n",
    "            'IS_SOUTHWEST_MONSOON': 1 if month in [6, 7, 8, 9] else 0,\n",
    "            'IS_NORTHEAST_MONSOON': 1 if month in [12, 1, 2, 3] else 0,\n",
    "        }\n",
    "        \n",
    "        for station in station_names:\n",
    "            station_dummy_col = f'STATION_{station}'\n",
    "            \n",
    "            # CRITICAL CHECK: Ensure the station has median features AND a corresponding dummy column\n",
    "            if station not in station_medians_dict or station_dummy_col not in station_cols:\n",
    "                 continue\n",
    "            \n",
    "            # 1. Start with station's median non-temporal features\n",
    "            station_base_features = station_medians_dict[station].copy()\n",
    "            \n",
    "            # 2. Overwrite with temporal features\n",
    "            station_base_features.update(temporal_features)\n",
    "            \n",
    "            # 3. Create a dictionary that contains ALL features in X_train\n",
    "            # Initialize all feature values to 0 (for dummies) or median (for continuous)\n",
    "            \n",
    "            # Start with a clean dictionary, populating all required features\n",
    "            feature_vector = {}\n",
    "            for col in X_train.columns:\n",
    "                if col.startswith('STATION_'):\n",
    "                    feature_vector[col] = 0\n",
    "                else:\n",
    "                    # Use a default value (like median) if a feature is somehow missing from the base dict\n",
    "                    feature_vector[col] = station_base_features.get(col, 0) \n",
    "            \n",
    "            # 4. Set the correct station dummy to 1\n",
    "            feature_vector[station_dummy_col] = 1\n",
    "            \n",
    "            # 5. Convert to DataFrame, ensuring order matches X_train\n",
    "            X_station = pd.DataFrame([feature_vector], columns=X_train.columns)\n",
    "            \n",
    "            # 6. Make predictions and build the result dictionary\n",
    "            day_predictions = {\n",
    "                'Date': date_str, \n",
    "                'Day': target_date.strftime(\"%A\"), \n",
    "                'Station': station,\n",
    "                'HI': models['HI'].predict(X_station)[0],\n",
    "                'TMAX': models['TMAX'].predict(X_station)[0],\n",
    "                'RH': models['RH'].predict(X_station)[0]\n",
    "            }\n",
    "            predictions_list.append(day_predictions)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df_pred = pd.DataFrame(predictions_list)\n",
    "\n",
    "    if df_pred.empty:\n",
    "        print(\"FATAL ERROR: Prediction DataFrame is empty.\")\n",
    "        print(\"Reason: No station successfully matched its historical features to the model's feature set.\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    return df_pred.sort_values(by=['Date', 'Station']).reset_index(drop=True)\n",
    "\n",
    "# Make 3-day predictions for all stations\n",
    "future_predictions_by_station = predict_next_days_by_station(models, df_processed, X_train, n_days=3)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# FINAL OUTPUT DISPLAY LOGIC (Unchanged, as it was already robust)\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "if future_predictions_by_station.empty:\n",
    "    print(\"Cannot display forecast: Prediction DataFrame is empty. See FATAL ERROR message above.\")\n",
    "else:\n",
    "    # Get the unique dates and their corresponding labels\n",
    "    unique_dates = future_predictions_by_station['Date'].unique()\n",
    "    period_labels = ['Today', 'Tomorrow', 'Day After Tomorrow']\n",
    "    date_to_period = dict(zip(unique_dates, period_labels[:len(unique_dates)]))\n",
    "\n",
    "    # --- TODAY'S FORECAST (first unique date) ---\n",
    "    today_date = unique_dates[0]\n",
    "    \n",
    "    print(f\"\\n--- {date_to_period[today_date]}'S FORECAST ({today_date}) (By Station) ---\")\n",
    "    today_forecast = future_predictions_by_station[\n",
    "        future_predictions_by_station['Date'] == today_date \n",
    "    ].sort_values(by='HI', ascending=False)\n",
    "\n",
    "    today_forecast_display = today_forecast[['Station', 'HI', 'TMAX', 'RH']].copy()\n",
    "    today_forecast_display['HI'] = today_forecast_display['HI'].round(1).astype(str) + '°C'\n",
    "    today_forecast_display['TMAX'] = today_forecast_display['TMAX'].round(1).astype(str) + '°C'\n",
    "    today_forecast_display['RH'] = today_forecast_display['RH'].round(1).astype(str) + '%'\n",
    "\n",
    "    print(tabulate(today_forecast_display, headers=['Station', 'Heat Index', 'Max Temp', 'Humidity'], \n",
    "                   tablefmt='pipe', showindex=False))\n",
    "\n",
    "    # --- 3-Day National Average Summary ---\n",
    "    print(\"\\n--- 3-Day National Average Summary (For Comparison) ---\")\n",
    "    daily_summary = future_predictions_by_station.groupby('Date').agg({\n",
    "        'HI': 'mean',\n",
    "        'TMAX': 'mean',\n",
    "        'RH': 'mean',\n",
    "        'Day': 'first'\n",
    "    }).reset_index()\n",
    "\n",
    "    daily_summary['Period'] = daily_summary['Date'].map(date_to_period)\n",
    "\n",
    "    daily_summary_display = daily_summary[['Period', 'Day', 'HI', 'TMAX', 'RH']].copy()\n",
    "    daily_summary_display['HI'] = daily_summary_display['HI'].round(1).astype(str) + '°C'\n",
    "    daily_summary_display['TMAX'] = daily_summary_display['TMAX'].round(1).astype(str) + '°C'\n",
    "    daily_summary_display['RH'] = daily_summary_display['RH'].round(1).astype(str) + '%'\n",
    "\n",
    "    print(tabulate(daily_summary_display, headers=['Period', 'Day', 'Avg. Heat Index', 'Avg. Max Temp', 'Avg. Humidity'], \n",
    "                   tablefmt='pipe', showindex=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352fc9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Check data types of station dummies\n",
    "station_dummies = pd.get_dummies(df_processed['STATION'], prefix='STATION', dummy_na=False)\n",
    "print(\"Station dummy data types:\")\n",
    "print(station_dummies.dtypes.iloc[:5])\n",
    "\n",
    "print(\"\\nSample values:\")\n",
    "print(station_dummies.iloc[0, :5])\n",
    "\n",
    "# Check if they are numeric\n",
    "print(f\"\\nAre station dummies numeric? {station_dummies.dtypes.apply(lambda x: x in ['int64', 'float64', 'int32', 'float32']).all()}\")\n",
    "\n",
    "# Fix: Convert to numeric explicitly\n",
    "station_dummies_numeric = station_dummies.astype(int)\n",
    "print(f\"\\nAfter conversion - data types:\")\n",
    "print(station_dummies_numeric.dtypes.iloc[:5])\n",
    "\n",
    "print(f\"\\nWould survive numeric filter: {len(station_dummies_numeric.select_dtypes(include=[np.number]).columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c66c932",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "# Assume 'X_test' contains the last available feature row(s)\n",
    "# and 'model' is the trained regressor for HI, TMAX, RH\n",
    "\n",
    "last_date = pd.to_datetime(f\"{int(data['YEAR'].iloc[-1])}-{int(data['MONTH'].iloc[-1])}-{int(data['DAY'].iloc[-1])}\")\n",
    "\n",
    "# Predict tomorrow and day after tomorrow\n",
    "future_dates = [last_date + timedelta(days=1), last_date + timedelta(days=2)]\n",
    "preds = model.predict(X_test.tail(1))\n",
    "\n",
    "# If predicting multiple targets, preds may be 2D\n",
    "if preds.ndim == 1:\n",
    "    preds = preds.reshape(1, -1)\n",
    "\n",
    "results = pd.DataFrame(preds, columns=['Pred_HI', 'Pred_TMAX', 'Pred_RH'])\n",
    "results['Date'] = future_dates\n",
    "\n",
    "# Reorder columns\n",
    "results = results[['Date', 'Pred_HI', 'Pred_TMAX', 'Pred_RH']]\n",
    "print(results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
